<!doctype html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133811886-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133811886-2');
  </script>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <script src="../static/js/nav.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" href="../static/css/main.css">
</head>
<body>

    <div class="site-wrapper">
        <div class="sticky top-bar">
      <div id="mySidenav" class="sidenav sticky" include-nav="/static/nav.html">
        </div>
        <span class="sticky hamburger" style="font-size:30px;cursor:pointer" onclick="openNav()">&#9776;</span>
    </div>
        <div class="container">
            <div class="blog-header">
                <h1 class="blog-title" id="top">kubernetes_incident_response</h1>
                <hr>
              </div>
        
              <div class="row">
        
                <div class="col-sm-8 blog-main">
                    <div class="blog-post" data-spy="scroll" data-target="#sidebar" data-offset="15">
                        <p>I've seen a lot of posts on how to get up and running with Kubernetes, and a lot of posts on how to make it more secure or how to spin up monitoring. One thing I have not seen is any posts on how to handle incident response once a cluster has been compromised. The goal of this post is to start to demistify steps that can be taken to perform IR on a cluster. This is not all in compassing, this is merely a starting point. Some of the questions we will be digging into in this post are:   
                            <ul>
                                <li>How does IR work in Kubernetes, how can we build an accurate picture of what happened, when it happened, and who did it?</li>
                                <li>Given the ephemeral nature of pods withing a cluster is IR even worth the effort?</li>
                                <li>Are there any overlaps with Linux IR?</li>
                                <li>What about cloud IR, any overlaps there?</li>
                            </ul>
                        </p>
                        <p>In an effort to make this more realistic we will disect behavior from activitiy I generated by working through the <a href="https://github.com/honk-ci/honkctl/tree/master/challenges/2020-march">March 2020 honkctl ctf</a>. During this post we will see manual enumeration activity as well as what some of the automated tools look like such as <a href="https://github.com/cyberark/kubesploit">kubesploit</a>, and <a href="https://github.com/aquasecurity/kube-hunter">kube-hunter</a></p>
                        <hr />


                        <h3 id="environment">Environment</h3>
                        <p>For this environment I spun up a simple single node cluster using kubeadm. I could have done this in Kind, but who doesn't have a good time spinning up a full cluster. In the cluster I have a splunk pod that is injesting two log sources (apiserver audit logs, and falco logs) from files on the host. Falco is installed as a service on the host and is running in its default configuration with the exception of a couple custom rules that were added. For the audit policy I initially had it super simple:</p>
                        <pre><code>
                            # Log all requests at the Metadata level.<br />
                            apiVersion: audit.k8s.io/v1<br />
                            kind: Policy<br />
                            rules:<br />
                              - level: RequestResponse
                        </code></pre>
                        <p>But it generated too much noise for the purposes of this post, and honestly it would probably never be configured this way in a real production environment either. I opted to use the sample policy Datadog has on one of there posts: <a href="https://www.datadoghq.com/blog/monitor-kubernetes-audit-logs/">How to monitor Kubernetes audit logs</a>.</p>

                            
                        <pre><code>
                            apiVersion: audit.k8s.io/v1<br />
                            kind: Policy<br />
                            rules:<br />
                            # do not log requests to the following <br />
                            - level: None<br />
                              nonResourceURLs:<br />
                              - "/healthz*"<br />
                              - "/logs"<br />
                              - "/metrics"<br />
                              - "/swagger*"<br />
                              - "/version"<br />
                              <br />
                            # limit level to Metadata so token is not included in the spec/status<br />
                            - level: Metadata<br />
                              omitStages:<br />
                              - RequestReceived<br />
                              resources:<br />
                              - group: authentication.k8s.io<br />
                                resources:<br />
                                - tokenreviews<br />
                                <br />
                            # extended audit of auth delegation<br />
                            - level: RequestResponse<br />
                              omitStages:<br />
                              - RequestReceived<br />
                              resources:<br />
                              - group: authorization.k8s.io<br />
                                resources:<br />
                                - subjectaccessreviews<br />
                                <br />
                            # log changes to pods at RequestResponse level<br />
                            - level: RequestResponse<br />
                              omitStages:<br />
                              - RequestReceived<br />
                              resources:<br />
                              - group: "" # core API group; add third-party API services and your API services if needed<br />
                                resources: ["pods"]<br />
                                verbs: ["create", "patch", "update", "delete"]<br />
                                <br />
                            # log everything else at Metadata level<br />
                            - level: Metadata<br />
                              omitStages:<br />
                              - RequestReceived<br />
                        </code></pre>    
                            
                            
                        <p>In an actual production environment your mileage may very with this policy, and I highly recommend tweaking this policy to meet your own needs.</p>

                        <p>Once the logging and monitoring was configured, we can setup the honkctl environment. Thankfully the creators have left the challenge manifests up and the instructions are pretty straight forward and we just have to run <code>kubectl apply -f </code> on each of the manifests folders. Note, the full instructions call for creating a kind cluster. Since we have a full cluster up and running with Kubeadm there is no need to leverage kind. Once all of the apply commands have been completed running pods look similar to this with the CTF enivonrment under the <code>pub</code> and <code>garden</code> namespaces.</p>

                        <pre><code>
                            <table>
                            <tr><td>NAMESPACE</td><td>NAME</td><td>READY</td><td>STATUS</td></tr>
                            <tr><td>default</td><td>net-tools</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>default</td><td>splunk-68985f65fd-bhmgk</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>garden</td><td>groundskeeper-66fcd95f7d-tzvw9</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>calico-kube-controllers-69496d8b75-9nlck&nbsp&nbsp</td><td>1/1</td><td>Running</tr>
                            <tr><td>kube-system&nbsp&nbsp</td><td>calico-node-5fgqn</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>coredns-74ff55c5b-8fzq9</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>coredns-74ff55c5b-zhht5</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>etcd-ubuntu</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>kube-apiserver-ubuntu</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>kube-controller-manager-ubuntu</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>kube-proxy-krq8q</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>kube-system</td><td>kube-scheduler-ubuntu</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>pub</td><td>barkeep-7494fd6774-wp2c9</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>pub</td><td>burly-man-bc55b7d69-9wxfp</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>pub</td><td>delivery-person-78db4bf478-wn7x4</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>pub</td><td>old-man-7698b7f856-stnbk</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>pub</td><td>woman-0</td><td>1/1</td><td>Running</td></tr>
                            <tr><td>pub</td><td>woman-1</td><td>1/1</td><td>Running</td></tr>
                            </table>
                        </code></pre>
                        <hr />


                        <h3 id="call">The Call</h3>
                        <p>A developer reaches out and mentions they have been seeing some odd behavior on the cluster for the past few days. Random pods have been created and they're not sure where they came from. We learn there are currently  two pods with unknown origins, <code>test</code> and <code>child</code>. Both containers are running in the <code>pub</code> namespace. </p>


                        <p>Next we'll look at the output from the <code>describe pod</code> command for each of the two pods. This will help us better understand what is going on inside the pods themselves.</p>

                        <p>Output from the describe pod test command:</p>
                        <img src="../static/images/kubernetes/ir/ir-1-describe-test.png" alt="describe-test" class="img-fluid">
                        <p>Output from the describe pod child command:</p>
                        <img src="../static/images/kubernetes/ir/ir-2-describe-child.png" alt="describe-child" class="img-fluid">
                        <p>Both pods each only contain one container, and they are both running the same image <code>raesene/alpine-nettools</code>. This container is full of common network utilities that could be helpful for attackers and also for Admins trying to troubleshoot something within a cluster. The test container does not seem to have any other worrysome configurations. The child pod however, has a <code>HostPath</code> mount for <code>/</code>. That is not good. It looks like the entire host file system has been mounted inside the container. Next we will pivot over to log sources to see what story they will tell.</p>
                        <hr />

                        <h3 id="logs">Logs</h3>
                        <p>As discussed earlier all of the cluster audit logs are being sent to Splunk. Since we know the pod names and the start time for them lets pull on that thread to see what created them. We can accomplish this with a search similar to this one:</p>
                        <pre><code>
                          index=honkctl-2020 (child OR test) verb=create 
                          |table _time, user.username, userAgent, requestURI
                        </code></pre>
                        <p>This produces 23 results. Pivoting to a stats count of just the <code>user.username</code> field and we get three results</p>
                        <img src="../static/images/kubernetes/ir/splunk-user-stats.PNG" alt="splunk-user-stats" class="img-fluid">
                        <p>Looking at these results it is weird that a serviceaccount, particularly the garden:goose account would have anything to do with creating resources with <code>child</code> or <code>test</code>. Lets narrow our focus a bit to look at some of the full events for the garden:goose service account using this search: <pre><code>index=honkctl-2020 (child OR test) verb=create "user.username"="system:serviceaccount:garden:goose"</code></pre></p>

                        <p>One of the first results we get back is below:</p>
                          
                        <pre><code>                       	
                        { [-]</br>
                        &emsp;annotations: { [+]</br>
                        &emsp;}</br>
                        &emsp;apiVersion: audit.k8s.io/v1</br>
                        &emsp;auditID: a546873a-5010-459c-be7c-afd4aaa3552f</br>
                        &emsp;impersonatedUser: { [+]</br>
                        &emsp;}</br>
                        &emsp;kind: Event</br>
                        &emsp;level: RequestResponse</br>
                        &emsp;objectRef: { [+]</br>
                        &emsp;}</br>
                        &emsp;requestObject: { [+]</br>
                        &emsp;}</br>
                        &emsp;requestReceivedTimestamp: 2021-04-11T16:25:10.272830Z</br>
                        &emsp;requestURI: /api/v1/namespaces/pub/pods?fieldManager=kubectl-run</br>
                        &emsp;responseObject: { [+]</br>
                        &emsp;}</br>
                        &emsp;responseStatus: { [+]</br>
                        &emsp;}</br>
                        &emsp;sourceIPs: [ [-]</br>
                        &emsp;&emsp;192.168.1.212</br>
                        &emsp;]</br>
                        &emsp;stage: ResponseComplete</br>
                        &emsp;stageTimestamp: 2021-04-11T16:25:10.284110Z</br>
                        &emsp;user: { [+]</br>
                        &emsp;}</br>
                        &emsp;userAgent: kubectl/v1.20.5 (linux/amd64) kubernetes/6b1d87a</br>
                        &emsp;verb: create</br>
                        }</br>
                        </code></pre>   
                        
                        <p>Looking at the requestURI field it tells us the <code>kubectl run</code> command was used to create a pod in the <code>pub</code> namespace. If we expand out the responseObject section we can see the pod created was called <code>test</code> and all of the same details that we discovered earlier about the pod when we ran the describe describe pod command earlier.</p>
                          <img src="../static/images/kubernetes/ir/splunk-test-pod-creation.PNG" alt="splunk-test-pod-creation" class="img-fluid"> 
                        

                          <p>Interestingly, if we look at the <code>impersonatedUser</code> section it appears the run command was issued while impersonating the groundskeeper service account.</p>
                          <img src="../static/images/kubernetes/ir/splunk-impersonation.PNG" alt="splunk-impersonation" class="img-fluid"> 
                          <p>With this information we can begin to piece together the command issued, which is expected to look similar to:</p>
                          <pre><code>
                            kubectl --as system:serviceaccount:garden:groundskeeper --namespace pub run --image=raesene/alpine-nettools
                          </code></pre>
                          <p>We are still missing what command was executed within the pod, if any. Let's keep digging in and see what else we can find.</p>
                          


                        <hr />
                        <h3 id="prevention">Prevention</h3>
                        <p>So, how do we prevent this. Where did the owner of this account go wrong? There are a few places where this could have be stopped. Lets dive into them.</p>
                        <ol>
                          <li>The most important is to not expose credentials. If we were not able to leverage raynors credentials we would not have been able to authenticate and further escalate privileges.</li>
                          <li>Limit what users can do. The policy associated with raynor allowed for too much access. I imagine in the real world a user with this set of actions at there disposal would likely have much more persmissions within the account. </li>
                        </ol>

                    </div><!-- /.blog-post -->
                </div><!-- /.blog-main -->

                    <!-- Blog Side Bar-->
                <div class="col-md-3" id=sidebar data-spy="affix-bottom" data-offset-top="0">

                    <nav class="post-sidebar hidden-print hidden-sm hidden-xs affix sticky" id="post-nav">
                        <h5>contents</h5>
                        <ul class="nav post-sidenav"> 
                            <li class=""> <a href="#raynor">Raynor</a> </li>
                            <li class=""> <a href="#versions">Versions</a> </li>
                            <li class=""> <a href="#prevention">Prevention</a></li>
                            <li class=""> <a href="#top">Top</a> </li>
                    </nav>
                </div><!-- /.blog-sidebar -->

            </div><!-- /.row -->
        </div> <!--/.container-->
    </div> <!--/.wrapper -->


<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
<script>includeHTML();</script>
</html> 